Google Inception Net首次出现在ILSVRC 2014的比赛中（和VGGNet同年），就以较大优势取得了第一名。它最大的特点是控制了计算量和参数量的同时，获得了非常好的分类性能——top-5错误率6.67%，只有AlexNet的一半不到。

Inception V1有22层深，比AlexNet的8层或者VGGNet的19层还要更深。但其计算量只有15亿次浮点运算，同时只有500万的参数量，仅为AlexNet参数量（6000万）的1/12，却可以达到远胜于AlexNet的准确率，可以说是非常优秀并且非常实用的模型。

Inception V1降低参数量的目的有两点：第一，参数越多模型越庞大，需要供模型学习的数据量就越大，而目前高质量的数据非常昂贵；第二，参数越多，耗费的计算资源也会更大。





!!!很重要：Inception V1参数少但效果好的原因除了模型层数更深、表达能力更强外，还有两点：
1/ 去除了最后的全连接层，用全局平均池化层（即将图片尺寸变为1*1）来取代它。全连接层几乎占据了AlexNet或VGGNet中90%的参数量，而且会引起过拟合，去除全连接层后模型训练更快并且减轻了过拟合。
2/ Inception V1中精心设计的Inception Module提高了参数的利用效率，这一部分也借鉴了Network In Network的思想，形象的解释就是Inception Module本身如同大网络中的一个小网络，其结构可以反复堆叠在一起形成大网络。（其结构图见网址https://blog.csdn.net/AMDS123/article/details/70232650）






我们再来看Inception Module的基本结构，其中有4个分支：
第一个分支对输入进行1*1的卷积，这其实也是NIN中提出的一个重要结构。1*1的卷积是一个非常优秀的结构，它可以跨通道组织信息，提高网络的表达能力，同时可以对输出通道升维和降维。可以看到Inception Module的4个分支都用到了1*1卷积，来进行低成本（计算量比3*3小很多）的跨通道的特征变换。
第二个分支先使用了1*1卷积，然后连接3*3卷积，相当于进行了两次特征变换。
第三个分支类似，先是1*1的卷积，然后连接5*5卷积。
最后一个分支则是3*3最大池化后直接使用1*1卷积。

Inception Module的4个分支在最后通过一个聚合操作合并（在输出通道数这个维度上聚合）。





同时，Google Inception Net还是一个大家族，包括Inception V1,V2,V3,V4....
1/ Inception V2学习了VGGNet，用两个3*3的卷积代替5*5的大卷积（用以降低参数量并减轻过拟合），还提出了著名的Batch Normalization（以下简称BN）方法。BN是一个非常有效的正则化方法，可以让大型卷积网络的训练速度加快很多倍，同时收敛后的分类准确率也可以得到大幅提高。
2/ BN在用于神经网络某层时，会对每一个mini-batch数据的内部进行标准化（normalization）处理，使输出规范化到N(0,1)的正态分布，减少了Internal Covariate Shift（内部神经元分布的改变）。BN的论文指出，传统的深度神经网络在训练时，每一层的输入的分布都在变化，导致训练变得困难，我们只能使用一个很小的学习速率解决这个问题。而对每一层使用BN之后，我们就可以有效地解决这个问题，学习速率可以增大很多倍，达到之前的准确率所需要的迭代次数只有1/14，训练时间大大缩短。

注意：而达到之前的准确率后，可以继续训练，并最终取得远超于Inception V1模型的性能——top-5错误率4.8%，已经优于人眼水平。因为BN某种意义上还起到了正则化的作用，所以可以减少或者取消Dropout，简化网络结构。