{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用到的技巧：\n",
    "\n",
    "### 数据增强，防止过拟合，增大数据集，可以训练更深的模型，提高预测率\n",
    "- cutout\n",
    "- Padding + RandomCrop\n",
    "- mixup (暂时不用)\n",
    "\n",
    "### data_loader \n",
    "- batch_size为2的幂次方,设置成2的幂性能优化,因为线程数通常是2^n这种数字\n",
    "- pin_memory=(torch.cuda.is_available()，数据加载，如果有可能，在GPU上完成，提高速率\n",
    "\n",
    "### nn.CELU代替nn.ReLU\n",
    "平滑的激活函数对于优化过程也很有帮助\n",
    "\n",
    "### 标签平滑\n",
    "标签平滑是提高分类问题中神经网络训练速度和泛化的一个成熟技巧\n",
    "\n",
    "### 用优化的SGD方法\n",
    "- SGD小批量，则速度一定比全批量快，在大数据集尤其如此\n",
    "- momentum 增加动量惯性\n",
    "- weight_decay 即L2 regularization的系数，正则化\n",
    "- nesterov = Ture，表示用nesterov momentum\n",
    "\n",
    "torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9, weight_decay = 5e-4, nesterov = True)\n",
    "\n",
    "带有动量的SGD本质：使用指数加权平均之后的梯度代替原梯度进行参数更新。详情请见 https://zhuanlan.zhihu.com/p/62585696\n",
    "\n",
    "NAG（Nesterov Accelerated Gradient）的本质：多考虑了目标函数的二阶导信息。详情请见 https://zhuanlan.zhihu.com/p/22810533\n",
    "\n",
    "### epoch设置和learning rate的实时更新\n",
    "lr 更偏向于先升后降，最开始学习率比较小，中间学习率很高，最后学习率很低很低，最开始是为了预热，也为了防止SGD出现nan。 详情请见：\n",
    "- 学习率先升后降： https://www.jiqizhixin.com/articles/041905\n",
    "- 1cycle策略： SGD方法，大学习率（中间）时，momentum较小（0.85），小学习率时，momentum较大（0.95）。这也印证了一个直觉：在训练中，我们希望 SGD 可以迅速调整（大学习率）到搜索平坦区域的方向上，因此就应该对新的梯度赋予更大的权重（momentum相对变小）。 详情请见 https://cloud.tencent.com/developer/article/1118029\n",
    "- 学习率调整的代码实现（提供包）： https://www.cnblogs.com/wanghui-garcia/p/10895397.html\n",
    "\n",
    "### 初始化\n",
    "xavier初始化： 为了使得网络中信息更好的流动，每一层输出的方差应该尽量相等。\n",
    "参考： \n",
    "- https://zhuanlan.zhihu.com/p/25110150\n",
    "- https://www.cnblogs.com/darkknightzh/p/8297793.html\n",
    "- 代码实现： https://blog.csdn.net/qq_24724109/article/details/82050402\n",
    "\n",
    "### 白化\n",
    "原理：https://www.cnblogs.com/robert-dlut/p/4211174.html\n",
    "实现：torchvision.transforms.LinearTransformation + np.linalg.eig\n",
    "\n",
    "### maxpool 在 activation 前面，效果一样，可以节约时间\n",
    "\n",
    "### Ghost批量归一（暂缺）\n",
    "\n",
    "归一化（bn）最合适的批量大小大概在32左右。但在批量大小比较大的时候，比如512，降低其大小会严重影响训练时间。不过这一问题可以通过对batch的子集分别进行批量归一来解决，这种方法称为“ghost”批量归一。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T22:25:09.688881Z",
     "start_time": "2019-11-25T22:25:09.017662Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 用tensorboard来可视化模型\n",
    "from tensorboardX import SummaryWriter  # writer就相当于一个日志，保存你要做图的所有信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T22:25:09.695855Z",
     "start_time": "2019-11-25T22:25:09.689878Z"
    }
   },
   "outputs": [],
   "source": [
    "# 设置随机数种子\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T22:25:09.979090Z",
     "start_time": "2019-11-25T22:25:09.865395Z"
    }
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "# Hyper-parameters for updating learning rate\n",
    "num_epoches = 40\n",
    "learning_rate = 0.1\n",
    "batch_size = 128\n",
    "\n",
    "# lr updating parameters\n",
    "epoches = [1, 3, 4, 20, 21, 30, 31, 37, 38, num_epoches]\n",
    "lrs = [learning_rate, 0.2, 0.1, 0.1, 0.01, 0.01, 0.001, 0.001, 0.0001, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T22:25:10.326162Z",
     "start_time": "2019-11-25T22:25:10.313196Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据增强方法：1.cutout\n",
    "class Cutout(object):\n",
    "    \"\"\"Randomly mask out one or more patches（补丁，就是一个遮挡小块） from an image.\n",
    "    Args:\n",
    "        n_holes (int): Number of patches to cut out of each image.\n",
    "        length (int): The length (in pixels) of each square patch.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_holes, length):\n",
    "        self.n_holes = n_holes\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (Tensor): Tensor image of size (C, H, W).\n",
    "        Returns:\n",
    "            Tensor: Image with n_holes of dimension length x length cut out of it.\n",
    "        \"\"\"\n",
    "        h = img.size(1)\n",
    "        w = img.size(2)\n",
    "\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "\n",
    "        for n in range(self.n_holes):\n",
    "            # (x,y)表示方形补丁的中心位置\n",
    "            y = np.random.randint(h)\n",
    "            x = np.random.randint(w)\n",
    "\n",
    "            y1 = np.clip(y - self.length // 2, 0, h)\n",
    "            y2 = np.clip(y + self.length // 2, 0, h)\n",
    "            x1 = np.clip(x - self.length // 2, 0, w)\n",
    "            x2 = np.clip(x + self.length // 2, 0, w)\n",
    "\n",
    "            mask[y1: y2, x1: x2] = 0.\n",
    "\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = mask.expand_as(img)\n",
    "        img = img * mask\n",
    "\n",
    "        return img\n",
    "\n",
    "# 2.Padding + RandomCrop,用transform自带函数做增强\n",
    "# 3.mixup也是一种简单的数据增强方法,应用地方是在训练的时候，详见 https://blog.csdn.net/winycg/article/details/88410981\n",
    "#   beta分布参见 https://www.cnblogs.com/think-and-do/p/6593809.html\n",
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha) # beta分布\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "# 4.label_smoothing 标签平滑是提高分类问题中神经网络训练速度和泛化的一个成熟技巧。\n",
    "# 在Inception论文中提出，对标签label进行增强，作者认为one-hot编码会过拟合，因此作者在交叉熵中对错误label也分配了很小的权重来防止过拟合。\n",
    "# 作者引入了参数ϵ, 详见 https://blog.csdn.net/winycg/article/details/88410981\n",
    "# 在label_smoothing之后，loss的数值可能会增大一些，这与acuracy无关\n",
    "def CrossEntropyLoss_label_smooth(outputs, targets, device,\n",
    "                                  num_classes=10, epsilon=0.075):\n",
    "    N = targets.size(0)\n",
    "    smoothed_labels = torch.full(size=(N, num_classes),\n",
    "                                 fill_value=epsilon / (num_classes - 1))\n",
    "    smoothed_labels.scatter_(dim=1, index=torch.unsqueeze(targets.to('cpu'), dim=1),\n",
    "                             value=1-epsilon)\n",
    "    smoothed_labels = smoothed_labels.to(device)\n",
    "    log_prob = nn.functional.log_softmax(outputs, dim=1)\n",
    "    loss = - torch.sum(log_prob * smoothed_labels) / N\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T22:25:14.512963Z",
     "start_time": "2019-11-25T22:25:10.779947Z"
    }
   },
   "outputs": [],
   "source": [
    "# CIFAR-10 dataset\n",
    "cifar_norm_mean = (0.49139968, 0.48215827, 0.44653124)\n",
    "cifar_norm_std = (0.24703233, 0.24348505, 0.26158768)\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                             train=True, \n",
    "                                             download=False,\n",
    "                                             transform=transforms.Compose([\n",
    "                                                 transforms.RandomCrop(32,padding = 4), #图像长宽周围都填充4个单位长度（像素）,\n",
    "                                                                        #（32，32）->（40，40）,然后随机切割，切割完后的尺寸为32*32\n",
    "                                                 transforms.RandomHorizontalFlip(),  #对PIL.image水平翻转，默认反转概率0.5\n",
    "                                                 transforms.ToTensor(),  # totensor要放在这个固定的位置，之前是image，之后是normalize\n",
    "                                                 transforms.Normalize(cifar_norm_mean, cifar_norm_std),\n",
    "                                                 Cutout(n_holes=1, length=16)])\n",
    "                                            )\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                            train=False, \n",
    "                                            transform=transforms.Compose([\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.Normalize(cifar_norm_mean, cifar_norm_std)])                                   \n",
    "                                            )\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,    # 设置成2的幂性能优化,因为线程数通常是2^n这种数字\n",
    "                                           shuffle=True,\n",
    "                                           #num_workers=2,\n",
    "                                           pin_memory=(torch.cuda.is_available()),\n",
    "                                           )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False,\n",
    "                                          #num_workers=2,\n",
    "                                          pin_memory=(torch.cuda.is_available()),\n",
    "                                          )\n",
    "\n",
    "# Cifar-10的标签\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "                                                 \n",
    "print(train_dataset,'\\n')\n",
    "print(train_dataset[0][0].shape,'\\n')            #第1个数据的X,这里目前由于是ToTensor的结果，是tensor，暂时没有被padding\n",
    "print(train_dataset[0][1],'\\n')            #第1个数据的y\n",
    "print(train_dataset.targets[0],'\\n')       #等同于第1个数据的y\n",
    "\n",
    "batch=next(iter(train_loader))\n",
    "images,labels = batch\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T21:02:51.981044Z",
     "start_time": "2019-11-24T21:02:51.977056Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3x3 convolution，定义3*3的卷积层（因为kernel_size为3）,步长默认为1，这样的话kernel_size为3，步长为1，最后图像的长宽不变\n",
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                     stride=stride, padding=1, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T21:02:52.413887Z",
     "start_time": "2019-11-24T21:02:52.396932Z"
    }
   },
   "outputs": [],
   "source": [
    "# Residual block\n",
    "class ResidualBlock(nn.Module): # stride=1时，(batch,in_c,w,h) -> (batch,out_ch_c,w,h)\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)  # (batch,in_c,w,h) -> (batch,out_c,w/stride,h/stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.celu = nn.CELU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)         # (batch,out_c,w,h) -> (batch,out_c,w,h)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.celu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:  # downsample的作用：仍然是一个3x3 conv+ bn\n",
    "            residual = self.downsample(x) \n",
    "        out += residual      # 相当于那个经典的图的x（如果有downsample,则是self.downsample）直接连到了输出，想一想残差原理，\n",
    "                             # 以及反向传递的时候避开了downsample走在主线上\n",
    "        out = self.celu(out)\n",
    "        return out\n",
    "    \n",
    "# ResNet\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):     #这个block传递的是ResidualBlock类,layers传递的是[2,2]\n",
    "        super(ResNet, self).__init__()\n",
    "        # prep\n",
    "        self.conv1 = conv3x3(3, 16)         #(batch,3,32,32) -> (batch,16,32,32)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.celu1 = nn.CELU(inplace=True)\n",
    "        \n",
    "        #layer1_ens\n",
    "        self.conv2 = conv3x3(16, 16)         #(batch,16,32,32) -> (batch,16,32,32)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.max_pool1 = nn.MaxPool2d(2,2)   # (batch,16,32,32) -> (batch,16,16,16)\n",
    "        self.celu2 = nn.CELU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block, 16, 32, layers[0], 2) #layers[0]为2，则最后有1+1个block,(b,16,16,16)->(这里有一个downsample)(b,32,8,8)->(b,32,8,8)\n",
    "        \n",
    "        # layer2_ens\n",
    "        self.conv3 = conv3x3(32, 64)         #(batch,32,8,8) -> (batch,64,8,8)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.max_pool2 = nn.MaxPool2d(2,2)   # (batch,64,8,8) -> (batch,64,4,4)\n",
    "        self.celu3 = nn.CELU(inplace=True)   # celu2如果不用celu1的，就新开辟了空间，画图时候会区分开\n",
    "        \n",
    "        # layer3_ens\n",
    "        self.conv4 = conv3x3(64, 64)         #(batch,64,4,4) -> (batch,64,4,4)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.max_pool3 = nn.MaxPool2d(2,2)   # (batch,64,2,2) -> (batch,64,2,2)\n",
    "        self.celu4 = nn.CELU(inplace=True)\n",
    "        self.layer2 = self.make_layer(block, 64, 128, layers[1], 2) #layers[0]为2，则最后有1+1个block,(b,64,2,2)->(这里有一个downsample)(b,128,1,1)->(b,128,1,1)\n",
    "        \n",
    "        #self.avg_pool = nn.AvgPool2d(4)           # kernel_size 为4,若输入是(c,w,h),则输出是(c,w-3,h-3),则(b,128,4,4) -> (b,128,1,1)\n",
    "                                                  # 这里最后用avg pool好，因为这里这4个数值元素更多的是平等关系，为了减小方差\n",
    "        self.fc = nn.Linear(128, num_classes)     # (b,128,1,1)经过view(out.size(0),-1)之后变成了(b,256),这里out.size(0)为b,再经过fc变成(b,10)\n",
    "        \n",
    "    def make_layer(self, block, in_channels, out_channels, blocks, stride=1): # blocks表示block的数目\n",
    "        downsample = None\n",
    "        if (stride != 1) or (in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        layers = []\n",
    "        layers.append(block(in_channels, out_channels, stride, downsample))\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # prep\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.celu1(out)\n",
    "        \n",
    "        #layer1_ens\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.max_pool1(out)\n",
    "        out = self.celu2(out)\n",
    "        out = self.layer1(out)\n",
    "        \n",
    "        # layer2_ens\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.max_pool2(out)\n",
    "        out = self.celu3(out)\n",
    "        \n",
    "        # layer3_ens\n",
    "        out = self.conv4(out)\n",
    "        out = self.bn4(out)\n",
    "        out = self.max_pool3(out)\n",
    "        out = self.celu4(out)\n",
    "        out = self.layer2(out)\n",
    "\n",
    "        #out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)      # 由(b,64,1,1) 变成了 (b,64)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T23:19:20.140605Z",
     "start_time": "2019-11-23T23:19:16.636969Z"
    }
   },
   "outputs": [],
   "source": [
    "model = ResNet(ResidualBlock, [2, 2]).to(device)\n",
    "\n",
    "# 保存模型结构\n",
    "dummy_input = torch.rand(20, 3, 32, 32).to(device)  # 假设输入20张3*32*32的图片\n",
    "with SummaryWriter(comment='Resnet9') as w:\n",
    "    w.add_graph(model, (dummy_input,))\n",
    "    \n",
    "print('# model parameters:', sum(param.numel() for param in model.parameters()))\n",
    "    \n",
    "# 如果要看模型，需要：\n",
    "# 1.在你的terminal（终端）中 ，在D:\\我的代码\\jupyter notebook\\Programs\\CIFAR10\\runs 地址下，使用以下命令\n",
    "# tensorboard --logdir Nov20_21-13-56_LAPTOP-F48B5MHEResnet9\n",
    "# 2.在浏览器输入\n",
    "# http://localhost:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T23:19:20.154568Z",
     "start_time": "2019-11-23T23:19:20.142600Z"
    }
   },
   "outputs": [],
   "source": [
    "# prediction function\n",
    "def pred_rate(preds,labels):\n",
    "    return preds.eq(labels).sum().item()/labels.shape[0]\n",
    "\n",
    "# For updating learning rate\n",
    "def piecewise_linear(optimizer, curr_epoch, epoches, lrs):  # 注意,epoches 是list，如[0,40,60],lrs也是list如[0.1,0.3,0],这里从0开始计算\n",
    "                                                        # 表示1~20epoch时，lr是0~0.4的线性升高，21~60epoch时，lr是0.4~0的线性下降\n",
    "    length = len(lrs)\n",
    "    for i in range (length-1):\n",
    "        if curr_epoch > epoches[i] and curr_epoch < epoches[i+1]:\n",
    "            lr = lrs[i] + (curr_epoch-epoches[i])/(epoches[i+1]-epoches[i])*(lrs[i+1]-lrs[i])\n",
    "            break\n",
    "        elif curr_epoch == epoches[i]:\n",
    "            lr = lrs[i]\n",
    "            break\n",
    "        elif curr_epoch == epoches[-1]:        # 考虑最后一个epoch的特殊情况\n",
    "            lr = lrs[-1]\n",
    "            break\n",
    "\n",
    "    # print\n",
    "    #if lrs[i] == lrs[i+1]:\n",
    "        #print('Epoch [{}/{}], learning rate kept still in {}'.format(epoch+1, num_epoches, lr))\n",
    "    #else:\n",
    "        #print('Epoch [{}/{}], learning rate updated to {}'.format(epoch+1, num_epoches, lr))\n",
    "    \n",
    "    # update lr\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T23:19:20.160552Z",
     "start_time": "2019-11-23T23:19:20.156562Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "# criterion 要写在后面\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9, weight_decay = 5e-4, nesterov = True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T16:09:46.585128Z",
     "start_time": "2019-11-23T16:09:46.463454Z"
    }
   },
   "source": [
    "# 实现混合精度 （FP16+FP32）\n",
    "from apex import amp\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\") # 这里是“欧一”，不是“零一”，表示混合精度\n",
    "with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "    scaled_loss.backward()\n",
    "    \n",
    "# 会报错： module 'torch.distributed' has no attribute 'is_initialized'。 \n",
    "# 原因：We don’t support Windows for torch.distributed, unfortunately. windows不支持分布计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T01:33:43.497873Z",
     "start_time": "2019-11-24T01:33:43.360241Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reload the model\n",
    "model = ResNet(ResidualBlock, [2, 2]).to(device)\n",
    "model.load_state_dict(torch.load('resnet9_piecewise_linear.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T01:23:10.757138Z",
     "start_time": "2019-11-24T00:59:00.805434Z"
    }
   },
   "outputs": [],
   "source": [
    "# 画图\n",
    "loss_ens = []\n",
    "lr_ens = [learning_rate]\n",
    "pred_rate_ens = []\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epoches):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images,labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = CrossEntropyLoss_label_smooth(outputs, labels, device)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 98 == 0:\n",
    "            preds=outputs.argmax(dim=1)\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Prediction rate: {:.4f}\"\n",
    "                   .format(epoch+1, num_epoches, i+1, total_step, loss.item(), pred_rate(preds,labels)))\n",
    "            \n",
    "            # 保存数据以画图\n",
    "            pred_rate_ens.append(pred_rate(preds,labels))\n",
    "            loss_ens.append(loss.item())\n",
    "            \n",
    "    # update learning rate\n",
    "    curr_lr = piecewise_linear(optimizer, epoch+1, epoches, lrs)\n",
    "    # 保存数据以画图\n",
    "    lr_ens.append(curr_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T01:33:47.616354Z",
     "start_time": "2019-11-24T01:33:47.400930Z"
    }
   },
   "outputs": [],
   "source": [
    "# draw the loss, pred_rate with iteration andd the lr with epoch\n",
    "plt. figure(figsize=(30,10))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.plot(np.arange(1,len(pred_rate_ens)+1),pred_rate_ens)\n",
    "plt.title('prediction rate')\n",
    "plt.xlabel('iteration')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.plot(np.arange(1,len(loss_ens)+1),loss_ens)\n",
    "plt.title('loss')\n",
    "plt.xlabel('iteration')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.plot(np.arange(num_epoches+1),lr_ens)\n",
    "plt.title('learning rate')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T01:26:11.720934Z",
     "start_time": "2019-11-24T01:26:11.692013Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the model checkpoint，保存的是模型的参数，则保存和读取跟直接保存模型不太一样,但是这个方法成本小\n",
    "torch.save(model.state_dict(), 'resnet9_piecewise_linear.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T01:26:13.979896Z",
     "start_time": "2019-11-24T01:26:11.875537Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model = ResNet(ResidualBlock, [2, 2]).to(device)\n",
    "model.load_state_dict(torch.load('resnet9_piecewise_linear.ckpt'))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():         # 使用 torch,no_grad()构建不需要track的上下文环境，这个时候再不会跟踪track各个tensor的梯度\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
