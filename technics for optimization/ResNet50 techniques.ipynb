{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T06:30:38.318871Z",
     "start_time": "2019-11-24T06:30:37.620630Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 用tensorboard来可视化模型\n",
    "from tensorboardX import SummaryWriter  # writer就相当于一个日志，保存你要做图的所有信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T12:09:00.832359Z",
     "start_time": "2019-11-23T12:09:00.822385Z"
    }
   },
   "source": [
    ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T11:37:05.233957Z",
     "start_time": "2019-11-24T11:37:05.228957Z"
    }
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "# Hyper-parameters for updating learning rate\n",
    "num_epochs = 250\n",
    "learning_rate = 0.1\n",
    "batch_size = 256\n",
    "\n",
    "# lr updating parameters\n",
    "epochs = [1, 3, 4, 100, 101, 180, 181, 230, 231, num_epochs]\n",
    "lrs = [learning_rate, 0.2, 0.1, 0.1, 0.01, 0.01, 0.001, 0.001, 0.0001, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T11:37:05.408478Z",
     "start_time": "2019-11-24T11:37:05.395512Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据增强方法：1.cutout\n",
    "class Cutout(object):\n",
    "    \"\"\"Randomly mask out one or more patches（补丁，就是一个遮挡小块） from an image.\n",
    "    Args:\n",
    "        n_holes (int): Number of patches to cut out of each image.\n",
    "        length (int): The length (in pixels) of each square patch.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_holes, length):\n",
    "        self.n_holes = n_holes\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (Tensor): Tensor image of size (C, H, W).\n",
    "        Returns:\n",
    "            Tensor: Image with n_holes of dimension length x length cut out of it.\n",
    "        \"\"\"\n",
    "        h = img.size(1)\n",
    "        w = img.size(2)\n",
    "\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "\n",
    "        for n in range(self.n_holes):\n",
    "            # (x,y)表示方形补丁的中心位置\n",
    "            y = np.random.randint(h)\n",
    "            x = np.random.randint(w)\n",
    "\n",
    "            y1 = np.clip(y - self.length // 2, 0, h)\n",
    "            y2 = np.clip(y + self.length // 2, 0, h)\n",
    "            x1 = np.clip(x - self.length // 2, 0, w)\n",
    "            x2 = np.clip(x + self.length // 2, 0, w)\n",
    "\n",
    "            mask[y1: y2, x1: x2] = 0.\n",
    "\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = mask.expand_as(img)\n",
    "        img = img * mask\n",
    "\n",
    "        return img\n",
    "\n",
    "# 2.Padding + RandomCrop,用transform自带函数做增强\n",
    "# 3.mixup也是一种简单的数据增强方法,应用地方是在训练的时候，详见 https://blog.csdn.net/winycg/article/details/88410981\n",
    "#   beta分布参见 https://www.cnblogs.com/think-and-do/p/6593809.html\n",
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha) # beta分布\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "# 4.label_smoothing 标签平滑是提高分类问题中神经网络训练速度和泛化的一个成熟技巧。\n",
    "# 在Inception论文中提出，对标签label进行增强，作者认为one-hot编码会过拟合，因此作者在交叉熵中对错误label也分配了很小的权重来防止过拟合。\n",
    "# 作者引入了参数ϵ, 详见 https://blog.csdn.net/winycg/article/details/88410981\n",
    "# 在label_smoothing之后，loss的数值可能会增大一些，这与acuracy无关\n",
    "def CrossEntropyLoss_label_smooth(outputs, targets, device,\n",
    "                                  num_classes=10, epsilon=0.075):\n",
    "    N = targets.size(0)\n",
    "    smoothed_labels = torch.full(size=(N, num_classes),\n",
    "                                 fill_value=epsilon / (num_classes - 1))\n",
    "    smoothed_labels.scatter_(dim=1, index=torch.unsqueeze(targets.to('cpu'), dim=1),\n",
    "                             value=1-epsilon)\n",
    "    smoothed_labels = smoothed_labels.to(device)\n",
    "    log_prob = nn.functional.log_softmax(outputs, dim=1)\n",
    "    loss = - torch.sum(log_prob * smoothed_labels) / N\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T11:37:06.808752Z",
     "start_time": "2019-11-24T11:37:05.665805Z"
    }
   },
   "outputs": [],
   "source": [
    "# CIFAR-10 dataset\n",
    "cifar_norm_mean = (0.49139968, 0.48215827, 0.44653124)\n",
    "cifar_norm_std = (0.24703233, 0.24348505, 0.26158768)\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                             train=True, \n",
    "                                             download=False,\n",
    "                                             transform=transforms.Compose([\n",
    "                                                 transforms.Pad(4),     #图像长宽周围都填充4个单位长度（像素）,（32，32）->（40，40）\n",
    "                                                 transforms.RandomCrop(32),         #随机切割，切割完后的尺寸为32*32\n",
    "                                                 transforms.RandomHorizontalFlip(),  #对PIL.image水平翻转，默认反转概率0.5\n",
    "                                                 transforms.ToTensor(),  # totensor要放在这个固定的位置，之前是image，之后是normalize\n",
    "                                                 transforms.Normalize(cifar_norm_mean, cifar_norm_std),\n",
    "                                                 Cutout(n_holes=1, length=16),])\n",
    "                                            )\n",
    "\n",
    "train_dataset，val_dataset = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                            train=False, \n",
    "                                            transform=transforms.Compose([\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.Normalize(cifar_norm_mean, cifar_norm_std)])                                   \n",
    "                                            )\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,    # 设置成2的幂性能优化,因为线程数通常是2^n这种数字\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=(torch.cuda.is_available()),\n",
    "                                           )\n",
    "\n",
    "val_loader = test_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=(torch.cuda.is_available()),\n",
    "                                          )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=(torch.cuda.is_available()),\n",
    "                                          )\n",
    "\n",
    "print(train_dataset,'\\n')\n",
    "print(train_dataset[0][0].shape,'\\n')            #第1个数据的X,这里目前由于是ToTensor的结果，是tensor，暂时没有被padding\n",
    "print(train_dataset[0][1],'\\n')            #第1个数据的y\n",
    "print(train_dataset.targets[0],'\\n')       #等同于第1个数据的y\n",
    "\n",
    "batch=next(iter(train_loader))\n",
    "images,labels = batch\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pin_memory 理解**\n",
    "pin_memory就是锁页内存，创建DataLoader时，设置pin_memory=True，则意味着生成的Tensor数据最开始是属于内存中的锁页内存，这样将内存的Tensor转义到GPU的显存就会更快一些。\n",
    "\n",
    "主机中的内存，有两种存在方式，一是锁页，二是不锁页，锁页内存存放的内容在任何情况下都不会与主机的虚拟内存进行交换（注：虚拟内存就是硬盘），而不锁页内存在主机内存不足时，数据会存放在虚拟内存中。\n",
    "\n",
    "而显卡中的显存全部是锁页内存！\n",
    "\n",
    "当计算机的内存充足的时候，可以设置pin_memory=True。当系统卡住，或者交换内存使用过多的时候，设置pin_memory=False。因为pin_memory与电脑硬件性能有关，pytorch开发者不能确保每一个炼丹玩家都有高端设备，因此pin_memory默认为False。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T11:37:07.191737Z",
     "start_time": "2019-11-24T11:37:06.810746Z"
    }
   },
   "outputs": [],
   "source": [
    "def Conv1(in_planes, places, stride=2): # (3,32,32)\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels=in_planes,out_channels=places,kernel_size=7,stride=stride,padding=3, bias=False), #(places,16,16)\n",
    "        nn.BatchNorm2d(places),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1),  # (places,8,8)\n",
    "        nn.CELU(inplace=True),\n",
    "    )\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self,in_places,places, stride=1,downsampling=False, expansion = 4):\n",
    "        super(Bottleneck,self).__init__()\n",
    "        self.expansion = expansion\n",
    "        self.downsampling = downsampling\n",
    "\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_places,out_channels=places,kernel_size=1,stride=1, bias=False),\n",
    "            nn.BatchNorm2d(places),\n",
    "            nn.CELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=places, out_channels=places, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(places),\n",
    "            nn.CELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=places, out_channels=places*self.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(places*self.expansion),\n",
    "        )\n",
    "\n",
    "        if self.downsampling:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_places, out_channels=places*self.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(places*self.expansion)\n",
    "            )\n",
    "        self.celu = nn.CELU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.bottleneck(x)\n",
    "\n",
    "        if self.downsampling:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.celu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, blocks, num_classes=1000, expansion = 4):  # (3,32,32)\n",
    "        super(ResNet,self).__init__()\n",
    "        self.expansion = expansion\n",
    "\n",
    "        self.conv1 = Conv1(in_planes = 3, places= 64)  # (64,8,8)\n",
    "\n",
    "        self.layer1 = self.make_layer(in_places = 64, places= 64, block=blocks[0], stride=1)   # (256,8,8)\n",
    "        self.layer2 = self.make_layer(in_places = 256,places=128, block=blocks[1], stride=2)   # (512,4,4)\n",
    "        self.layer3 = self.make_layer(in_places=512,places=256, block=blocks[2], stride=2)     # (1024,2,2)\n",
    "        self.layer4 = self.make_layer(in_places=1024,places=512, block=blocks[3], stride=2)    # (2048,1,1)\n",
    "\n",
    "        #self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(2048,num_classes)\n",
    "        \n",
    "        # 初始化\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def make_layer(self, in_places, places, block, stride):\n",
    "        layers = []\n",
    "        layers.append(Bottleneck(in_places, places,stride, downsampling =True))\n",
    "        for i in range(1, block):\n",
    "            layers.append(Bottleneck(places*self.expansion, places))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        #x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "model = ResNet([3, 4, 6, 3], num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T11:38:59.773769Z",
     "start_time": "2019-11-24T11:38:59.306019Z"
    }
   },
   "outputs": [],
   "source": [
    "# 保存模型结构\n",
    "dummy_input = torch.rand(20, 3, 32, 32).to(device)  # 假设输入20张3*32*32的图片\n",
    "with SummaryWriter(comment='Resnet50') as w:\n",
    "    w.add_graph(model, (dummy_input,))\n",
    "    \n",
    "print('# model parameters:', sum(param.numel() for param in model.parameters()))\n",
    "    \n",
    "# 如果要看模型，需要：\n",
    "# 1.在你的terminal（终端）中 ，在D:\\我的代码\\jupyter notebook\\Programs\\CIFAR10\\runs 地址下，使用以下命令\n",
    "# tensorboard --logdir Nov20_21-13-56_LAPTOP-F48B5MHEResnet9\n",
    "# 2.在浏览器输入\n",
    "# http://localhost:6006"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T09:52:34.915470Z",
     "start_time": "2019-11-24T09:52:34.911462Z"
    }
   },
   "source": [
    "# 分布式训练\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T11:39:01.813948Z",
     "start_time": "2019-11-24T11:39:01.807963Z"
    }
   },
   "outputs": [],
   "source": [
    "# prediction function for simple data\n",
    "def pred_rate(preds,labels):\n",
    "    return preds.eq(labels).sum().item()/labels.shape[0]\n",
    "\n",
    "# prediction function for data loader\n",
    "def pred_rate_loader(model, val_loader, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():         # 使用 torch,no_grad()构建不需要track的上下文环境，这个时候再不会跟踪track各个tensor的梯度\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loss = []\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, dim=1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            loss.append(CrossEntropyLoss_label_smooth(outputs, labels, device).cpu().numpy())\n",
    "            \n",
    "    model.train()\n",
    "    return np.mean(loss), correct / total      # 返回loss和pred acc\n",
    "\n",
    "# For updating learning rate\n",
    "def piecewise_linear(optimizer, curr_epoch, epochs, lrs):  # 注意,epochs 是list，如[0,40,60],lrs也是list如[0.1,0.3,0],这里从0开始计算\n",
    "                                                        # 表示1~20epoch时，lr是0~0.4的线性升高，21~60epoch时，lr是0.4~0的线性下降\n",
    "    length = len(lrs)\n",
    "    for i in range (length-1):\n",
    "        if curr_epoch > epochs[i] and curr_epoch < epochs[i+1]:\n",
    "            lr = lrs[i] + (curr_epoch-epochs[i])/(epochs[i+1]-epochs[i])*(lrs[i+1]-lrs[i])\n",
    "            break\n",
    "        elif curr_epoch == epochs[i]:\n",
    "            lr = lrs[i]\n",
    "            break\n",
    "        elif curr_epoch == epochs[-1]:        # 考虑最后一个epoch的特殊情况\n",
    "            lr = lrs[-1]\n",
    "            break\n",
    "\n",
    "    # print\n",
    "    #if lrs[i] == lrs[i+1]:\n",
    "        #print('Epoch [{}/{}], learning rate kept still in {}'.format(epoch+1, num_epochs, lr))\n",
    "    #else:\n",
    "        #print('Epoch [{}/{}], learning rate updated to {}'.format(epoch+1, num_epochs, lr))\n",
    "    \n",
    "    # update lr\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T11:39:03.733203Z",
     "start_time": "2019-11-24T11:39:03.729213Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "# criterion 要写在后面\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9, weight_decay = 5e-4, nesterov = True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 实现混合精度 （FP16+FP32）\n",
    "from apex import amp\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\") # 这里是“欧一”，不是“零一”，表示混合精度\n",
    "with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "    scaled_loss.backward()\n",
    "    \n",
    "# 会报错： module 'torch.distributed' has no attribute 'is_initialized'。 \n",
    "# 原因：We don’t support Windows for torch.distributed, unfortunately. windows不支持分布计算"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Reload the model\n",
    "model = ResNet(ResidualBlock, [2, 2]).to(device)\n",
    "model.load_state_dict(torch.load('resnet9_piecewise_linear.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T15:43:50.325973Z",
     "start_time": "2019-11-24T11:39:21.261273Z"
    }
   },
   "outputs": [],
   "source": [
    "# 画图\n",
    "loss_ens = []\n",
    "lr_ens = []\n",
    "pred_rate_ens = []\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images,labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = CrossEntropyLoss_label_smooth(outputs, labels, device)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 98 == 0:\n",
    "            curr_loss, curr_acc = pred_rate_loader(model, val_loader, device)\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Prediction rate: {:.4f}\"\n",
    "                   .format(epoch+1, num_epoches, i+1, total_step, curr_loss, curr_acc))\n",
    "            \n",
    "            # 保存数据以画图\n",
    "            pred_rate_ens.append(curr_acc)\n",
    "            loss_ens.append(curr_loss)\n",
    "            \n",
    "    # update learning rate\n",
    "    curr_lr = piecewise_linear(optimizer, epoch+1, epochs, lrs)\n",
    "    # 保存数据以画图\n",
    "    lr_ens.append(curr_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T15:44:05.342401Z",
     "start_time": "2019-11-24T15:44:04.996825Z"
    }
   },
   "outputs": [],
   "source": [
    "# draw the loss, pred_rate with iteration andd the lr with epoch\n",
    "plt. figure(figsize=(15,15))\n",
    "\n",
    "plt.subplot(311)\n",
    "plt.plot(np.arange(1,len(pred_rate_ens)+1),pred_rate_ens)\n",
    "plt.title('prediction rate')\n",
    "plt.xlabel('iteration')\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.plot(np.arange(1,len(loss_ens)+1),loss_ens)\n",
    "plt.title('loss')\n",
    "plt.xlabel('iteration')\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.plot(np.arange(num_epochs),lr_ens)\n",
    "plt.title('learning rate')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T15:44:12.292205Z",
     "start_time": "2019-11-24T15:44:12.148996Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the model checkpoint，保存的是模型的参数，则保存和读取跟直接保存模型不太一样,但是这个方法成本小\n",
    "torch.save(model.state_dict(), 'resnet50_piecewise_linear.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T15:44:16.023658Z",
     "start_time": "2019-11-24T15:44:12.587333Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test the model\n",
    "#model = models.resnet50().to(device)\n",
    "#model.fc = nn.Linear(512, 10).to(device)  # 层都需要放到device上面\n",
    "\n",
    "#model.load_state_dict(torch.load('resnet50_piecewise_linear.ckpt'))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():         # 使用 torch,no_grad()构建不需要track的上下文环境，这个时候再不会跟踪track各个tensor的梯度\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
