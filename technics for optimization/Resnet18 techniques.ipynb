{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T19:35:07.844657Z",
     "start_time": "2019-11-27T19:35:06.915318Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA   # 白化\n",
    "\n",
    "# 用tensorboard来可视化模型\n",
    "from tensorboardX import SummaryWriter  # writer就相当于一个日志，保存你要做图的所有信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T19:35:07.853633Z",
     "start_time": "2019-11-27T19:35:07.845654Z"
    }
   },
   "outputs": [],
   "source": [
    "# 设置随机数种子\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T19:35:07.965342Z",
     "start_time": "2019-11-27T19:35:07.854646Z"
    }
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "# Hyper-parameters for updating learning rate\n",
    "num_epochs = 75\n",
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "is_whitened = True\n",
    "\n",
    "# lr updating parameters\n",
    "epochs = [1, 3, 4, 30, 31, 50, 51, 70, 71, num_epochs]\n",
    "lrs = [learning_rate, 0.2, 0.1, 0.1, 0.01, 0.01, 0.001, 0.001, 0.0001, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T19:35:07.981299Z",
     "start_time": "2019-11-27T19:35:07.966339Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据增强方法：1.cutout\n",
    "class Cutout(object):\n",
    "    \"\"\"Randomly mask out one or more patches（补丁，就是一个遮挡小块） from an image.\n",
    "    Args:\n",
    "        n_holes (int): Number of patches to cut out of each image.\n",
    "        length (int): The length (in pixels) of each square patch.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_holes, length):\n",
    "        self.n_holes = n_holes\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (Tensor): Tensor image of size (C, H, W).\n",
    "        Returns:\n",
    "            Tensor: Image with n_holes of dimension length x length cut out of it.\n",
    "        \"\"\"\n",
    "        h = img.size(1)\n",
    "        w = img.size(2)\n",
    "\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "\n",
    "        for n in range(self.n_holes):\n",
    "            # (x,y)表示方形补丁的中心位置\n",
    "            y = np.random.randint(h)\n",
    "            x = np.random.randint(w)\n",
    "\n",
    "            y1 = np.clip(y - self.length // 2, 0, h)\n",
    "            y2 = np.clip(y + self.length // 2, 0, h)\n",
    "            x1 = np.clip(x - self.length // 2, 0, w)\n",
    "            x2 = np.clip(x + self.length // 2, 0, w)\n",
    "\n",
    "            mask[y1: y2, x1: x2] = 0.\n",
    "\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = mask.expand_as(img)\n",
    "        img = img * mask\n",
    "\n",
    "        return img\n",
    "# 2.Padding + RandomCrop,用transform自带函数做增强\n",
    "# 3.mixup也是一种简单的数据增强方法,应用地方是在训练的时候，详见 https://blog.csdn.net/winycg/article/details/88410981\n",
    "#   beta分布参见 https://www.cnblogs.com/think-and-do/p/6593809.html\n",
    "# 4.label_smoothing 标签平滑是提高分类问题中神经网络训练速度和泛化的一个成熟技巧。\n",
    "# 在Inception论文中提出，对标签label进行增强，作者认为one-hot编码会过拟合，因此作者在交叉熵中对错误label也分配了很小的权重来防止过拟合。\n",
    "# 作者引入了参数ϵ, 详见 https://blog.csdn.net/winycg/article/details/88410981\n",
    "# 在label_smoothing之后，loss的数值可能会增大一些，这与acuracy无关\n",
    "def CrossEntropyLoss_label_smooth(outputs, targets, device,       # outputs (bs,10)   targets(bs,)\n",
    "                                  num_classes=10, epsilon=0.075):\n",
    "    N = targets.size(0)\n",
    "    smoothed_labels = torch.full(size=(N, num_classes),\n",
    "                                 fill_value=epsilon / (num_classes - 1))\n",
    "    smoothed_labels.scatter_(dim=1, index=torch.unsqueeze(targets.to('cpu'), dim=1),  # torch.unsqueeze(..,dim=1),在维度1处，扩展维度\n",
    "                             value=1-epsilon) \n",
    "                                          # dim=1表示以行(第一维度)为单位，要填充的数为value，填充的位置为index，这里index维度（bs,1）\n",
    "                                          # 在smoothed_labels(维度(bs，10))上面扩充，对应的位置用1-epsilon覆盖之前的epsilon/9...\n",
    "    smoothed_labels = smoothed_labels.to(device)\n",
    "    log_prob = nn.functional.log_softmax(outputs, dim=1)   # (bs,)\n",
    "    loss = - torch.sum(log_prob * smoothed_labels) / N\n",
    "    return loss\n",
    "\n",
    "# 5 白化 可以提前对数据处理，然后保存数据\n",
    "def whiten(imgs,epsilon=1e-1):                             # imgs:(N,H,W,C),int数据类型\n",
    "    N,H,W,C = imgs.shape\n",
    "    imgs_flatten = imgs.reshape(N,-1)                     # imgs_flatten(N,H,W,C)\n",
    "    \n",
    "    imgs_norm = imgs_flatten/255.\n",
    "    imgs_norm = imgs_norm - imgs_norm.mean(axis=0)        # axis=0表示对每一列求平均值\n",
    "    cov = np.cov(imgs_norm, rowvar=True)                  # rowvar=True by default\n",
    "    U,S,V = np.linalg.svd(cov)\n",
    "    imgs_ZCA = U.dot(np.diag(1.0/np.sqrt(S + epsilon))).dot(U.T).dot(imgs_norm)\n",
    "    imgs_ZCA_rescaled = (imgs_ZCA - imgs_ZCA.min()) / (imgs_ZCA.max() - imgs_ZCA.min())\n",
    "\n",
    "    imgs_whitened = imgs_ZCA_rescaled.reshape(N,H,W,C)\n",
    "    imgs_whitened = (imgs_whitened * 255).astype(int)\n",
    "    return imgs_whitened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T19:35:38.374357Z",
     "start_time": "2019-11-27T19:35:08.503520Z"
    }
   },
   "outputs": [],
   "source": [
    "# CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data',train=True, download=False)\n",
    "\n",
    "# 不想让val_dataset有各种各样的transforms\n",
    "#val_dataset.transforms = transforms.Compose([transforms.ToTensor(),\n",
    "#                                             transforms.Normalize(cifar_norm_mean, cifar_norm_std)])                                  \n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data',train=False, download=False)\n",
    "\n",
    "# 保存array形式数据\n",
    "train_data = train_dataset.data\n",
    "train_targets = np.array(train_dataset.targets)\n",
    "test_data = test_dataset.data\n",
    "test_targets = np.array(test_dataset.targets)\n",
    "\n",
    "# 对数据做PCA白化,要分开做，不然一次不能分配50000*50000的矩阵那么大的空间\n",
    "train_data_whitened = np.zeros(train_data.shape,dtype = int)\n",
    "test_data_whitened = np.zeros(test_data.shape,dtype = int)\n",
    "\n",
    "# epsilon 和 image batch number，whitening的效果将会由这两个超参数决定。epsilon 越低，batch number越大，白化效果越严重\n",
    "if is_whitened:\n",
    "    for i in range(50):\n",
    "        train_data_whitened[i*1000:(i+1)*1000] = whiten(train_data[i*1000:(i+1)*1000],epsilon=1)\n",
    "    for i in range(10):\n",
    "        test_data_whitened[i*1000:(i+1)*1000] = whiten(test_data[i*1000:(i+1)*1000],epsilon=1)\n",
    "else:\n",
    "    train_data_whitened = train_data\n",
    "    test_data_whitened = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T21:48:36.950840Z",
     "start_time": "2019-11-26T21:48:36.277640Z"
    }
   },
   "outputs": [],
   "source": [
    "# 查看白化效果\n",
    "plt.figure(figsize=(20,4))\n",
    "for i in range(20):\n",
    "    plt.subplot(2,10,i+1)\n",
    "    if i < 10 :\n",
    "        plt.imshow(train_data_whitened[i])\n",
    "        plt.title('{}'.format(train_targets[i]))\n",
    "        plt.axis('off')\n",
    "    else:\n",
    "        plt.imshow(train_data[i-10])\n",
    "        plt.title('{}'.format(train_targets[i-10]))\n",
    "        plt.axis('off')\n",
    "plt.show()        \n",
    "\n",
    "# 设置成channel first，之前直接读取torchvision.datasets.CIFAR10，会自动变成channel first，\n",
    "# 但是转换成numpy时候，又会自动变成channel last，因为numpy就是这么约定俗成的，因此这里要自己设置channel first为后面做准备\n",
    "train_data_whitened = np.transpose(train_data_whitened, (0, 3, 1, 2))\n",
    "test_data_whitened = np.transpose(test_data_whitened, (0, 3, 1, 2))\n",
    "\n",
    "# 变成tensor\n",
    "train_data_whitened = torch.from_numpy(train_data_whitened).type(torch.float32)\n",
    "train_targets = torch.from_numpy(train_targets).type(torch.int64)\n",
    "test_data_whitened = torch.from_numpy(test_data_whitened).type(torch.float32)\n",
    "test_targets = torch.from_numpy(test_targets).type(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T21:48:36.966797Z",
     "start_time": "2019-11-26T21:48:36.952834Z"
    }
   },
   "outputs": [],
   "source": [
    "# transformations\n",
    "cifar_norm_mean = (0.49139968, 0.48215827, 0.44653124)\n",
    "cifar_norm_std = (0.24703233, 0.24348505, 0.26158768)\n",
    "train_transforms = transforms.Compose([\n",
    "                                    transforms.Pad(4),     #图像长宽周围都填充4个单位长度（像素）,（32，32）->（40，40）\n",
    "                                    transforms.RandomCrop(32),         #随机切割，切割完后的尺寸为32*32\n",
    "                                    transforms.RandomHorizontalFlip(),  #对PIL.image水平翻转，默认反转概率0.5\n",
    "                                    transforms.ToTensor(),  # totensor要放在这个固定的位置，之前是image，之后是normalize\n",
    "                                    transforms.Normalize(cifar_norm_mean, cifar_norm_std),\n",
    "                                    Cutout(n_holes=1, length=16)])\n",
    "test_transforms = transforms.Compose([\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(cifar_norm_mean, cifar_norm_std)])  \n",
    "\n",
    "# Dataset\n",
    "# 注意，这里的TensorDataset与之前的dataset方法不太一样\n",
    "# 比如，要查看train_dataset的第一个数据，直接看train_dataset[0]，其第1个train_X为train_dataset[0][0],第1个train_y为train_dataset[0][1]\n",
    "# 没有data和targets函数接口\n",
    "train_dataset = torch.utils.data.TensorDataset(train_data_whitened,train_targets)\n",
    "train_dataset.transforms = train_transforms\n",
    "train_dataset_sub, val_dataset = torch.utils.data.random_split(train_dataset, [45000, 5000]) #train_dataset, val_dataset是subset\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(test_data_whitened,test_targets)\n",
    "test_dataset.transforms = test_transforms\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset_sub,\n",
    "                                           batch_size=batch_size,    # 设置成2的幂性能优化,因为线程数通常是2^n这种数字\n",
    "                                           shuffle=True,\n",
    "                                           #num_workers=2,\n",
    "                                           pin_memory=(torch.cuda.is_available()),\n",
    "                                           )\n",
    "\n",
    "val_loader = test_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False,\n",
    "                                          #num_workers=2,\n",
    "                                          pin_memory=(torch.cuda.is_available()),\n",
    "                                          )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False,\n",
    "                                          #num_workers=2,\n",
    "                                          pin_memory=(torch.cuda.is_available()),\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T21:48:39.161008Z",
     "start_time": "2019-11-26T21:48:36.968792Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inchannel, outchannel, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "            nn.CELU(inplace=True),\n",
    "            nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel)\n",
    "        )\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or inchannel != outchannel:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(outchannel)\n",
    "            )\n",
    "        self.celu = nn.CELU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        out += self.shortcut(x)\n",
    "        out = self.celu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, ResidualBlock, num_classes=10):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.inchannel = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.CELU(),\n",
    "        )\n",
    "        self.layer1 = self.make_layer(ResidualBlock, 64,  2, stride=1)\n",
    "        self.layer2 = self.make_layer(ResidualBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self.make_layer(ResidualBlock, 256, 2, stride=2)\n",
    "        self.layer4 = self.make_layer(ResidualBlock, 512, 2, stride=2)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        self.avg_pool = nn.AvgPool2d(4)\n",
    "        \n",
    "        # 初始化\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)   #strides=[1,1]\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.inchannel, channels, stride))\n",
    "            self.inchannel = channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)        # (64,32,32)\n",
    "        out = self.layer1(out)     # (64,32,32)\n",
    "        out = self.layer2(out)     # (128,16,16)\n",
    "        out = self.layer3(out)     # (256,8,8)\n",
    "        out = self.layer4(out)     # (512,4,4)\n",
    "        out = self.avg_pool(out)   # (512,1,1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "model = ResNet18(ResidualBlock).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T21:48:41.761049Z",
     "start_time": "2019-11-26T21:48:39.161999Z"
    }
   },
   "outputs": [],
   "source": [
    "# 保存模型结构\n",
    "dummy_input = torch.rand(20, 3, 32, 32).to(device)  # 假设输入20张3*32*32的图片\n",
    "with SummaryWriter(comment='Resnet18') as w:\n",
    "    w.add_graph(model, (dummy_input,))\n",
    "    \n",
    "print('# model parameters:', sum(param.numel() for param in model.parameters()))\n",
    "    \n",
    "# 如果要看模型，需要：\n",
    "# 1.在你的terminal（终端）中 ，在D:\\我的代码\\jupyter notebook\\Programs\\CIFAR10\\runs 地址下，使用以下命令\n",
    "# tensorboard --logdir Nov20_21-13-56_LAPTOP-F48B5MHEResnet9\n",
    "# 2.在浏览器输入\n",
    "# http://localhost:6006"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 分布式训练\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T21:48:41.773018Z",
     "start_time": "2019-11-26T21:48:41.762047Z"
    }
   },
   "outputs": [],
   "source": [
    "# prediction function for simple data\n",
    "def pred_rate(preds,labels):\n",
    "    return preds.eq(labels).sum().item()/labels.shape[0]\n",
    "\n",
    "# prediction function for data loader\n",
    "def pred_rate_loader(model, val_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():         # 使用 torch,no_grad()构建不需要track的上下文环境，这个时候再不会跟踪track各个tensor的梯度\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loss = []\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, dim=1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            loss.append(CrossEntropyLoss_label_smooth(outputs, labels, device).cpu().numpy())\n",
    "            \n",
    "    model.train()\n",
    "    return np.mean(loss), correct / total      # 返回loss和pred acc\n",
    "\n",
    "# For updating learning rate and momentum\n",
    "def piecewise_linear(optimizer, curr_epoch, epochs, lrs):  # 注意,epochs 是list，如[1,40,60],lrs也是list如[0.1,0.3,0],这里从0开始计算\n",
    "                                                        # 表示1~20epoch时，lr是0~0.4的线性升高，21~60epoch时，lr是0.4~0的线性下降\n",
    "    length = len(lrs)\n",
    "    for i in range (length-1):\n",
    "        if curr_epoch > epochs[i] and curr_epoch < epochs[i+1]:\n",
    "            lr = lrs[i] + (curr_epoch-epochs[i])/(epochs[i+1]-epochs[i])*(lrs[i+1]-lrs[i])\n",
    "            break\n",
    "        elif curr_epoch == epochs[i]:\n",
    "            lr = lrs[i]\n",
    "            break\n",
    "        elif curr_epoch == epochs[-1]:        # 考虑最后一个epoch的特殊情况\n",
    "            lr = lrs[-1]\n",
    "            break\n",
    "    \n",
    "    # update lr\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    # update momentum\n",
    "    if lr <= 0.001:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['momentum'] = 0.95\n",
    "        \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T21:48:41.778004Z",
     "start_time": "2019-11-26T21:48:41.774014Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "# criterion 要写在后面\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.85, weight_decay = 5e-4, nesterov = True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 实现混合精度 （FP16+FP32）\n",
    "from apex import amp\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\") # 这里是“欧一”，不是“零一”，表示混合精度\n",
    "with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "    scaled_loss.backward()\n",
    "    \n",
    "# 会报错： module 'torch.distributed' has no attribute 'is_initialized'。 \n",
    "# 原因：We don’t support Windows for torch.distributed, unfortunately. windows不支持分布计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T21:48:41.781994Z",
     "start_time": "2019-11-26T21:48:41.779002Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reload the model\n",
    "#model = ResNet18(ResidualBlock).to(device)\n",
    "#model.load_state_dict(torch.load('resnet18_piecewise_linear.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T23:12:14.292170Z",
     "start_time": "2019-11-26T21:48:41.782993Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 画图准备\n",
    "loss_ens = []\n",
    "lr_ens = []\n",
    "pred_rate_ens = []\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images,labels = images.to(device), labels.to(device)                 # labels(bs,)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)                         # outputs(bs,10)\n",
    "        loss = CrossEntropyLoss_label_smooth(outputs, labels, device)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 88 == 0:\n",
    "            curr_loss, curr_acc = pred_rate_loader(model, val_loader, device)\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Prediction rate: {:.4f}\"\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, curr_loss, curr_acc))\n",
    "            \n",
    "            # 保存数据以画图\n",
    "            pred_rate_ens.append(curr_acc)\n",
    "            loss_ens.append(curr_loss)\n",
    "            \n",
    "    # update learning rate\n",
    "    curr_lr = piecewise_linear(optimizer, epoch+1, epochs, lrs)\n",
    "    # 保存数据以画图\n",
    "    lr_ens.append(curr_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T23:12:14.816768Z",
     "start_time": "2019-11-26T23:12:14.294164Z"
    }
   },
   "outputs": [],
   "source": [
    "# draw the loss, pred_rate with iteration andd the lr with epoch\n",
    "plt. figure(figsize=(15,15))\n",
    "\n",
    "plt.subplot(311)\n",
    "plt.plot(np.arange(1,len(pred_rate_ens)+1),pred_rate_ens)\n",
    "plt.title('prediction rate')\n",
    "plt.xlabel('iteration')\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.plot(np.arange(1,len(loss_ens)+1),loss_ens)\n",
    "plt.title('loss')\n",
    "plt.xlabel('iteration')\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.plot(np.arange(num_epochs),lr_ens)\n",
    "plt.title('learning rate')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T23:12:14.820757Z",
     "start_time": "2019-11-26T23:12:14.817764Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the model checkpoint，保存的是模型的参数，则保存和读取跟直接保存模型不太一样,但是这个方法成本小\n",
    "# torch.save(model.state_dict(), 'resnet18_piecewise_linear.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T23:12:18.323391Z",
     "start_time": "2019-11-26T23:12:14.821753Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test the model\n",
    "#model = models.resnet18().to(device)\n",
    "#model.fc = nn.Linear(512, 10).to(device)  # 层都需要放到device上面\n",
    "\n",
    "#model.load_state_dict(torch.load('resnet50_piecewise_linear.ckpt'))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():         # 使用 torch,no_grad()构建不需要track的上下文环境，这个时候再不会跟踪track各个tensor的梯度\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
