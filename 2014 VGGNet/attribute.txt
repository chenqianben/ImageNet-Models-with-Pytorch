VGGNet探索了卷积神经网络的深度与其性能之间的关系，通过反复堆叠3*3的小型卷积核和2*2的最大池化层，VGGNet成功地构筑了16~19层深的卷积神经网络
VGGNet论文中全部使用了3*3的卷积核和2*2的池化核，通过不断加深网络结构来提升性能。



结构： 
    前面有很多3*3卷积层，可以分成几个block，后面是3个全连接层。如果增加block的复杂度，网络的参数量并没有增长很多，这是因为参数量主要都消耗在最后3个全连接层。前面的卷积部分虽然很深，但是消耗的参数量不大，不过训练比较耗时的部分依然是卷积，因其计算量比较大。
    VGGNet标准情况下拥有5段卷积，每一段内有2~3个卷积层，同时每段尾部会连接一个最大池化层用来缩小图片尺寸。每段内的卷积核数量一样，越靠后的段的卷积核数量越多：64 – 128 – 256 – 512 – 512。其中经常出现多个完全一样的3*3的卷积层堆叠在一起的情况，这其实是非常有用的设计。



原理：
    两个3*3的卷积层串联相当于1个5*5的卷积层（想想为什么，可以画图），即一个像素会跟周围5*5的像素产生关联，可以说感受野大小为5*5。而3个3*3的卷积层串联的效果则相当于1个7*7的卷积层。除此之外，3个串联的3*3的卷积层，拥有比1个7*7的卷积层更少的参数量，只有后者的55%。
    最重要的是，3个3*3的卷积层拥有比1个7*7的卷积层更多的非线性变换（前者可以使用三次ReLU激活函数，而后者只有一次），使得CNN对特征的学习能力更强。



作者在对比各级网络时总结出了以下几个观点：
    1/ LRN层作用不大。
    2/ 越深的网络效果越好。
    3/ 1*1的卷积也是很有效的，但是没有3*3的卷积好，大一些的卷积核可以学习更大的空间特征。