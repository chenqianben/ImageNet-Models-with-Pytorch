{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T06:30:38.318871Z",
     "start_time": "2019-11-24T06:30:37.620630Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 用tensorboard来可视化模型\n",
    "from tensorboardX import SummaryWriter  # writer就相当于一个日志，保存你要做图的所有信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T11:37:05.233957Z",
     "start_time": "2019-11-24T11:37:05.228957Z"
    }
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "# Hyper-parameters for updating learning rate\n",
    "num_epochs = 250\n",
    "learning_rate = 0.1\n",
    "batch_size = 256\n",
    "\n",
    "# lr updating parameters\n",
    "epochs = [1, 3, 4, 100, 101, 180, 181, 230, 231, num_epochs]\n",
    "lrs = [learning_rate, 0.2, 0.1, 0.1, 0.01, 0.01, 0.001, 0.001, 0.0001, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T11:37:05.408478Z",
     "start_time": "2019-11-24T11:37:05.395512Z"
    }
   },
   "outputs": [],
   "source": [
    "class Cutout(object):\n",
    "    \"\"\"Randomly mask out one or more patches（补丁，就是一个遮挡小块） from an image.\n",
    "    Args:\n",
    "        n_holes (int): Number of patches to cut out of each image.\n",
    "        length (int): The length (in pixels) of each square patch.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_holes, length):\n",
    "        self.n_holes = n_holes\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (Tensor): Tensor image of size (C, H, W).\n",
    "        Returns:\n",
    "            Tensor: Image with n_holes of dimension length x length cut out of it.\n",
    "        \"\"\"\n",
    "        h = img.size(1)\n",
    "        w = img.size(2)\n",
    "\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "\n",
    "        for n in range(self.n_holes):\n",
    "            y = np.random.randint(h)\n",
    "            x = np.random.randint(w)\n",
    "\n",
    "            y1 = np.clip(y - self.length // 2, 0, h)\n",
    "            y2 = np.clip(y + self.length // 2, 0, h)\n",
    "            x1 = np.clip(x - self.length // 2, 0, w)\n",
    "            x2 = np.clip(x + self.length // 2, 0, w)\n",
    "\n",
    "            mask[y1: y2, x1: x2] = 0.\n",
    "\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = mask.expand_as(img)\n",
    "        img = img * mask\n",
    "\n",
    "        return img\n",
    "\n",
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha) # beta分布\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def CrossEntropyLoss_label_smooth(outputs, targets, device,\n",
    "                                  num_classes=10, epsilon=0.075):\n",
    "    N = targets.size(0)\n",
    "    smoothed_labels = torch.full(size=(N, num_classes),\n",
    "                                 fill_value=epsilon / (num_classes - 1))\n",
    "    smoothed_labels.scatter_(dim=1, index=torch.unsqueeze(targets.to('cpu'), dim=1),\n",
    "                             value=1-epsilon)\n",
    "    smoothed_labels = smoothed_labels.to(device)\n",
    "    log_prob = nn.functional.log_softmax(outputs, dim=1)\n",
    "    loss = - torch.sum(log_prob * smoothed_labels) / N\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T11:37:06.808752Z",
     "start_time": "2019-11-24T11:37:05.665805Z"
    }
   },
   "outputs": [],
   "source": [
    "# CIFAR-10 dataset\n",
    "cifar_norm_mean = (0.49139968, 0.48215827, 0.44653124)\n",
    "cifar_norm_std = (0.24703233, 0.24348505, 0.26158768)\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                             train=True, \n",
    "                                             download=False,\n",
    "                                             transform=transforms.Compose([\n",
    "                                                 transforms.Pad(4),     \n",
    "                                                 transforms.RandomCrop(32),       \n",
    "                                                 transforms.RandomHorizontalFlip(),  \n",
    "                                                 transforms.ToTensor(),  \n",
    "                                                 transforms.Normalize(cifar_norm_mean, cifar_norm_std),\n",
    "                                                 Cutout(n_holes=1, length=16),])\n",
    "                                            )\n",
    "\n",
    "train_dataset，val_dataset = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                            train=False, \n",
    "                                            transform=transforms.Compose([\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.Normalize(cifar_norm_mean, cifar_norm_std)])                                   \n",
    "                                            )\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,    \n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=(torch.cuda.is_available()),\n",
    "                                           )\n",
    "\n",
    "val_loader = test_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=(torch.cuda.is_available()),\n",
    "                                          )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=(torch.cuda.is_available()),\n",
    "                                          )\n",
    "\n",
    "print(train_dataset,'\\n')\n",
    "print(train_dataset[0][0].shape,'\\n')            \n",
    "print(train_dataset[0][1],'\\n')            \n",
    "print(train_dataset.targets[0],'\\n')      \n",
    "\n",
    "batch=next(iter(train_loader))\n",
    "images,labels = batch\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T11:37:07.191737Z",
     "start_time": "2019-11-24T11:37:06.810746Z"
    }
   },
   "outputs": [],
   "source": [
    "def Conv1(in_planes, places, stride=2): # (3,32,32)\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels=in_planes,out_channels=places,kernel_size=7,stride=stride,padding=3, bias=False), #(places,16,16)\n",
    "        nn.BatchNorm2d(places),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1),  # (places,8,8)\n",
    "        nn.CELU(inplace=True),\n",
    "    )\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self,in_places,places, stride=1,downsampling=False, expansion = 4):\n",
    "        super(Bottleneck,self).__init__()\n",
    "        self.expansion = expansion\n",
    "        self.downsampling = downsampling\n",
    "\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_places,out_channels=places,kernel_size=1,stride=1, bias=False),\n",
    "            nn.BatchNorm2d(places),\n",
    "            nn.CELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=places, out_channels=places, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(places),\n",
    "            nn.CELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=places, out_channels=places*self.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(places*self.expansion),\n",
    "        )\n",
    "\n",
    "        if self.downsampling:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_places, out_channels=places*self.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(places*self.expansion)\n",
    "            )\n",
    "        self.celu = nn.CELU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.bottleneck(x)\n",
    "\n",
    "        if self.downsampling:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.celu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, blocks, num_classes=1000, expansion = 4):  # (3,32,32)\n",
    "        super(ResNet,self).__init__()\n",
    "        self.expansion = expansion\n",
    "\n",
    "        self.conv1 = Conv1(in_planes = 3, places= 64)  # (64,8,8)\n",
    "\n",
    "        self.layer1 = self.make_layer(in_places = 64, places= 64, block=blocks[0], stride=1)   # (256,8,8)\n",
    "        self.layer2 = self.make_layer(in_places = 256,places=128, block=blocks[1], stride=2)   # (512,4,4)\n",
    "        self.layer3 = self.make_layer(in_places=512,places=256, block=blocks[2], stride=2)     # (1024,2,2)\n",
    "        self.layer4 = self.make_layer(in_places=1024,places=512, block=blocks[3], stride=2)    # (2048,1,1)\n",
    "\n",
    "        #self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(2048,num_classes)\n",
    "        \n",
    "        # 初始化\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def make_layer(self, in_places, places, block, stride):\n",
    "        layers = []\n",
    "        layers.append(Bottleneck(in_places, places,stride, downsampling =True))\n",
    "        for i in range(1, block):\n",
    "            layers.append(Bottleneck(places*self.expansion, places))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        #x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "model = ResNet([3, 4, 6, 3], num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T11:38:59.773769Z",
     "start_time": "2019-11-24T11:38:59.306019Z"
    }
   },
   "outputs": [],
   "source": [
    "# 保存模型结构\n",
    "dummy_input = torch.rand(20, 3, 32, 32).to(device)  \n",
    "with SummaryWriter(comment='Resnet50') as w:\n",
    "    w.add_graph(model, (dummy_input,))\n",
    "    \n",
    "print('# model parameters:', sum(param.numel() for param in model.parameters()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T09:52:34.915470Z",
     "start_time": "2019-11-24T09:52:34.911462Z"
    }
   },
   "source": [
    "# 分布式训练\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T11:39:01.813948Z",
     "start_time": "2019-11-24T11:39:01.807963Z"
    }
   },
   "outputs": [],
   "source": [
    "# prediction function for simple data\n",
    "def pred_rate(preds,labels):\n",
    "    return preds.eq(labels).sum().item()/labels.shape[0]\n",
    "\n",
    "# prediction function for data loader\n",
    "def pred_rate_loader(model, val_loader, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():         \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loss = []\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, dim=1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            loss.append(CrossEntropyLoss_label_smooth(outputs, labels, device).cpu().numpy())\n",
    "            \n",
    "    model.train()\n",
    "    return np.mean(loss), correct / total      \n",
    "\n",
    "# For updating learning rate\n",
    "def piecewise_linear(optimizer, curr_epoch, epochs, lrs):  \n",
    "    length = len(lrs)\n",
    "    for i in range (length-1):\n",
    "        if curr_epoch > epochs[i] and curr_epoch < epochs[i+1]:\n",
    "            lr = lrs[i] + (curr_epoch-epochs[i])/(epochs[i+1]-epochs[i])*(lrs[i+1]-lrs[i])\n",
    "            break\n",
    "        elif curr_epoch == epochs[i]:\n",
    "            lr = lrs[i]\n",
    "            break\n",
    "        elif curr_epoch == epochs[-1]:       \n",
    "            lr = lrs[-1]\n",
    "            break\n",
    "\n",
    "    # print\n",
    "    #if lrs[i] == lrs[i+1]:\n",
    "        #print('Epoch [{}/{}], learning rate kept still in {}'.format(epoch+1, num_epochs, lr))\n",
    "    #else:\n",
    "        #print('Epoch [{}/{}], learning rate updated to {}'.format(epoch+1, num_epochs, lr))\n",
    "    \n",
    "    # update lr\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T11:39:03.733203Z",
     "start_time": "2019-11-24T11:39:03.729213Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9, weight_decay = 5e-4, nesterov = True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 实现混合精度 （FP16+FP32）\n",
    "from apex import amp\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\") # 这里是“欧一”，不是“零一”，表示混合精度\n",
    "with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "    scaled_loss.backward()\n",
    "    \n",
    "# 会报错： module 'torch.distributed' has no attribute 'is_initialized'。 \n",
    "# 原因：We don’t support Windows for torch.distributed, unfortunately. windows不支持分布计算"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Reload the model\n",
    "model = ResNet(ResidualBlock, [2, 2]).to(device)\n",
    "model.load_state_dict(torch.load('resnet9_piecewise_linear.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T15:43:50.325973Z",
     "start_time": "2019-11-24T11:39:21.261273Z"
    }
   },
   "outputs": [],
   "source": [
    "# draw results\n",
    "loss_ens = []\n",
    "lr_ens = []\n",
    "pred_rate_ens = []\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images,labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = CrossEntropyLoss_label_smooth(outputs, labels, device)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 98 == 0:\n",
    "            curr_loss, curr_acc = pred_rate_loader(model, val_loader, device)\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Prediction rate: {:.4f}\"\n",
    "                   .format(epoch+1, num_epoches, i+1, total_step, curr_loss, curr_acc))\n",
    "            \n",
    "            pred_rate_ens.append(curr_acc)\n",
    "            loss_ens.append(curr_loss)\n",
    "            \n",
    "    # update learning rate\n",
    "    curr_lr = piecewise_linear(optimizer, epoch+1, epochs, lrs)\n",
    "    lr_ens.append(curr_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T15:44:05.342401Z",
     "start_time": "2019-11-24T15:44:04.996825Z"
    }
   },
   "outputs": [],
   "source": [
    "# draw the loss, pred_rate with iteration andd the lr with epoch\n",
    "plt. figure(figsize=(15,15))\n",
    "\n",
    "plt.subplot(311)\n",
    "plt.plot(np.arange(1,len(pred_rate_ens)+1),pred_rate_ens)\n",
    "plt.title('prediction rate')\n",
    "plt.xlabel('iteration')\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.plot(np.arange(1,len(loss_ens)+1),loss_ens)\n",
    "plt.title('loss')\n",
    "plt.xlabel('iteration')\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.plot(np.arange(num_epochs),lr_ens)\n",
    "plt.title('learning rate')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T15:44:12.292205Z",
     "start_time": "2019-11-24T15:44:12.148996Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the model checkpoint，\n",
    "torch.save(model.state_dict(), 'resnet50_piecewise_linear.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T15:44:16.023658Z",
     "start_time": "2019-11-24T15:44:12.587333Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test the model\n",
    "#model = models.resnet50().to(device)\n",
    "#model.fc = nn.Linear(512, 10).to(device)  # 层都需要放到device上面\n",
    "\n",
    "#model.load_state_dict(torch.load('resnet50_piecewise_linear.ckpt'))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():        \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
