{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T22:25:09.688881Z",
     "start_time": "2019-11-25T22:25:09.017662Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorboardX import SummaryWriter  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T22:25:09.695855Z",
     "start_time": "2019-11-25T22:25:09.689878Z"
    }
   },
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T22:25:09.979090Z",
     "start_time": "2019-11-25T22:25:09.865395Z"
    }
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "# Hyper-parameters for updating learning rate\n",
    "num_epoches = 40\n",
    "learning_rate = 0.1\n",
    "batch_size = 128\n",
    "\n",
    "# lr updating parameters\n",
    "epoches = [1, 3, 4, 20, 21, 30, 31, 37, 38, num_epoches]\n",
    "lrs = [learning_rate, 0.2, 0.1, 0.1, 0.01, 0.01, 0.001, 0.001, 0.0001, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T22:25:10.326162Z",
     "start_time": "2019-11-25T22:25:10.313196Z"
    }
   },
   "outputs": [],
   "source": [
    "class Cutout(object):\n",
    "    \"\"\"Randomly mask out one or more patches（补丁，就是一个遮挡小块） from an image.\n",
    "    Args:\n",
    "        n_holes (int): Number of patches to cut out of each image.\n",
    "        length (int): The length (in pixels) of each square patch.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_holes, length):\n",
    "        self.n_holes = n_holes\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (Tensor): Tensor image of size (C, H, W).\n",
    "        Returns:\n",
    "            Tensor: Image with n_holes of dimension length x length cut out of it.\n",
    "        \"\"\"\n",
    "        h = img.size(1)\n",
    "        w = img.size(2)\n",
    "\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "\n",
    "        for n in range(self.n_holes):\n",
    "            y = np.random.randint(h)\n",
    "            x = np.random.randint(w)\n",
    "\n",
    "            y1 = np.clip(y - self.length // 2, 0, h)\n",
    "            y2 = np.clip(y + self.length // 2, 0, h)\n",
    "            x1 = np.clip(x - self.length // 2, 0, w)\n",
    "            x2 = np.clip(x + self.length // 2, 0, w)\n",
    "\n",
    "            mask[y1: y2, x1: x2] = 0.\n",
    "\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = mask.expand_as(img)\n",
    "        img = img * mask\n",
    "\n",
    "        return img\n",
    "\n",
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha) # beta分布\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def CrossEntropyLoss_label_smooth(outputs, targets, device,\n",
    "                                  num_classes=10, epsilon=0.075):\n",
    "    N = targets.size(0)\n",
    "    smoothed_labels = torch.full(size=(N, num_classes),\n",
    "                                 fill_value=epsilon / (num_classes - 1))\n",
    "    smoothed_labels.scatter_(dim=1, index=torch.unsqueeze(targets.to('cpu'), dim=1),\n",
    "                             value=1-epsilon)\n",
    "    smoothed_labels = smoothed_labels.to(device)\n",
    "    log_prob = nn.functional.log_softmax(outputs, dim=1)\n",
    "    loss = - torch.sum(log_prob * smoothed_labels) / N\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T22:25:14.512963Z",
     "start_time": "2019-11-25T22:25:10.779947Z"
    }
   },
   "outputs": [],
   "source": [
    "# CIFAR-10 dataset\n",
    "cifar_norm_mean = (0.49139968, 0.48215827, 0.44653124)\n",
    "cifar_norm_std = (0.24703233, 0.24348505, 0.26158768)\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                             train=True, \n",
    "                                             download=False,\n",
    "                                             transform=transforms.Compose([\n",
    "                                                 transforms.RandomCrop(32,padding = 4), \n",
    "                                                 transforms.RandomHorizontalFlip(),  \n",
    "                                                 transforms.ToTensor(),  \n",
    "                                                 transforms.Normalize(cifar_norm_mean, cifar_norm_std),\n",
    "                                                 Cutout(n_holes=1, length=16)])\n",
    "                                            )\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                            train=False, \n",
    "                                            transform=transforms.Compose([\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.Normalize(cifar_norm_mean, cifar_norm_std)])                                   \n",
    "                                            )\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,   \n",
    "                                           shuffle=True,\n",
    "                                           #num_workers=2,\n",
    "                                           pin_memory=(torch.cuda.is_available()),\n",
    "                                           )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False,\n",
    "                                          #num_workers=2,\n",
    "                                          pin_memory=(torch.cuda.is_available()),\n",
    "                                          )\n",
    "\n",
    "# Cifar-10的标签\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "                                                 \n",
    "print(train_dataset,'\\n')\n",
    "print(train_dataset[0][0].shape,'\\n')           \n",
    "print(train_dataset[0][1],'\\n')           \n",
    "print(train_dataset.targets[0],'\\n')      \n",
    "\n",
    "batch=next(iter(train_loader))\n",
    "images,labels = batch\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T21:02:51.981044Z",
     "start_time": "2019-11-24T21:02:51.977056Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3x3 convolution\n",
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                     stride=stride, padding=1, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T21:02:52.413887Z",
     "start_time": "2019-11-24T21:02:52.396932Z"
    }
   },
   "outputs": [],
   "source": [
    "# Residual block\n",
    "class ResidualBlock(nn.Module): # stride=1时，(batch,in_c,w,h) -> (batch,out_ch_c,w,h)\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)  # (batch,in_c,w,h) -> (batch,out_c,w/stride,h/stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.celu = nn.CELU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)         # (batch,out_c,w,h) -> (batch,out_c,w,h)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.celu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:  # downsample的作用：仍然是一个3x3 conv+ bn\n",
    "            residual = self.downsample(x) \n",
    "        out += residual      \n",
    "        out = self.celu(out)\n",
    "        return out\n",
    "    \n",
    "# ResNet\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):     \n",
    "        super(ResNet, self).__init__()\n",
    "        # prep\n",
    "        self.conv1 = conv3x3(3, 16)         #(batch,3,32,32) -> (batch,16,32,32)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.celu1 = nn.CELU(inplace=True)\n",
    "        \n",
    "        #layer1_ens\n",
    "        self.conv2 = conv3x3(16, 16)         #(batch,16,32,32) -> (batch,16,32,32)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.max_pool1 = nn.MaxPool2d(2,2)   # (batch,16,32,32) -> (batch,16,16,16)\n",
    "        self.celu2 = nn.CELU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block, 16, 32, layers[0], 2) #layers[0]为2，则最后有1+1个block,(b,16,16,16)->(这里有一个downsample)(b,32,8,8)->(b,32,8,8)\n",
    "        \n",
    "        # layer2_ens\n",
    "        self.conv3 = conv3x3(32, 64)         #(batch,32,8,8) -> (batch,64,8,8)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.max_pool2 = nn.MaxPool2d(2,2)   # (batch,64,8,8) -> (batch,64,4,4)\n",
    "        self.celu3 = nn.CELU(inplace=True)   \n",
    "        \n",
    "        # layer3_ens\n",
    "        self.conv4 = conv3x3(64, 64)         #(batch,64,4,4) -> (batch,64,4,4)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.max_pool3 = nn.MaxPool2d(2,2)   # (batch,64,2,2) -> (batch,64,2,2)\n",
    "        self.celu4 = nn.CELU(inplace=True)\n",
    "        self.layer2 = self.make_layer(block, 64, 128, layers[1], 2) #layers[0]为2，则最后有1+1个block,(b,64,2,2)->(这里有一个downsample)(b,128,1,1)->(b,128,1,1)\n",
    "        \n",
    "        #self.avg_pool = nn.AvgPool2d(4)          \n",
    "        self.fc = nn.Linear(128, num_classes)     \n",
    "        \n",
    "    def make_layer(self, block, in_channels, out_channels, blocks, stride=1): \n",
    "        downsample = None\n",
    "        if (stride != 1) or (in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        layers = []\n",
    "        layers.append(block(in_channels, out_channels, stride, downsample))\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # prep\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.celu1(out)\n",
    "        \n",
    "        #layer1_ens\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.max_pool1(out)\n",
    "        out = self.celu2(out)\n",
    "        out = self.layer1(out)\n",
    "        \n",
    "        # layer2_ens\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.max_pool2(out)\n",
    "        out = self.celu3(out)\n",
    "        \n",
    "        # layer3_ens\n",
    "        out = self.conv4(out)\n",
    "        out = self.bn4(out)\n",
    "        out = self.max_pool3(out)\n",
    "        out = self.celu4(out)\n",
    "        out = self.layer2(out)\n",
    "\n",
    "        #out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)      # (b,64,1,1) ->(b,64)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T23:19:20.140605Z",
     "start_time": "2019-11-23T23:19:16.636969Z"
    }
   },
   "outputs": [],
   "source": [
    "model = ResNet(ResidualBlock, [2, 2]).to(device)\n",
    "\n",
    "dummy_input = torch.rand(20, 3, 32, 32).to(device) \n",
    "with SummaryWriter(comment='Resnet9') as w:\n",
    "    w.add_graph(model, (dummy_input,))\n",
    "    \n",
    "print('# model parameters:', sum(param.numel() for param in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T23:19:20.154568Z",
     "start_time": "2019-11-23T23:19:20.142600Z"
    }
   },
   "outputs": [],
   "source": [
    "# prediction function\n",
    "def pred_rate(preds,labels):\n",
    "    return preds.eq(labels).sum().item()/labels.shape[0]\n",
    "\n",
    "# For updating learning rate\n",
    "def piecewise_linear(optimizer, curr_epoch, epoches, lrs): \n",
    "    length = len(lrs)\n",
    "    for i in range (length-1):\n",
    "        if curr_epoch > epoches[i] and curr_epoch < epoches[i+1]:\n",
    "            lr = lrs[i] + (curr_epoch-epoches[i])/(epoches[i+1]-epoches[i])*(lrs[i+1]-lrs[i])\n",
    "            break\n",
    "        elif curr_epoch == epoches[i]:\n",
    "            lr = lrs[i]\n",
    "            break\n",
    "        elif curr_epoch == epoches[-1]:       \n",
    "            lr = lrs[-1]\n",
    "            break\n",
    "\n",
    "    # print\n",
    "    #if lrs[i] == lrs[i+1]:\n",
    "        #print('Epoch [{}/{}], learning rate kept still in {}'.format(epoch+1, num_epoches, lr))\n",
    "    #else:\n",
    "        #print('Epoch [{}/{}], learning rate updated to {}'.format(epoch+1, num_epoches, lr))\n",
    "    \n",
    "    # update lr\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T23:19:20.160552Z",
     "start_time": "2019-11-23T23:19:20.156562Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9, weight_decay = 5e-4, nesterov = True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T16:09:46.585128Z",
     "start_time": "2019-11-23T16:09:46.463454Z"
    }
   },
   "source": [
    "# 实现混合精度 （FP16+FP32）\n",
    "from apex import amp\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\") # 这里是“欧一”，不是“零一”，表示混合精度\n",
    "with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "    scaled_loss.backward()\n",
    "    \n",
    "# 会报错： module 'torch.distributed' has no attribute 'is_initialized'。 \n",
    "# 原因：We don’t support Windows for torch.distributed, unfortunately. windows不支持分布计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T01:33:43.497873Z",
     "start_time": "2019-11-24T01:33:43.360241Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reload the model\n",
    "model = ResNet(ResidualBlock, [2, 2]).to(device)\n",
    "model.load_state_dict(torch.load('resnet9_piecewise_linear.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T01:23:10.757138Z",
     "start_time": "2019-11-24T00:59:00.805434Z"
    }
   },
   "outputs": [],
   "source": [
    "# 画图\n",
    "loss_ens = []\n",
    "lr_ens = [learning_rate]\n",
    "pred_rate_ens = []\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epoches):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images,labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = CrossEntropyLoss_label_smooth(outputs, labels, device)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 98 == 0:\n",
    "            preds=outputs.argmax(dim=1)\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Prediction rate: {:.4f}\"\n",
    "                   .format(epoch+1, num_epoches, i+1, total_step, loss.item(), pred_rate(preds,labels)))\n",
    "            \n",
    "            # 保存数据以画图\n",
    "            pred_rate_ens.append(pred_rate(preds,labels))\n",
    "            loss_ens.append(loss.item())\n",
    "            \n",
    "    # update learning rate\n",
    "    curr_lr = piecewise_linear(optimizer, epoch+1, epoches, lrs)\n",
    "    # 保存数据以画图\n",
    "    lr_ens.append(curr_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T01:33:47.616354Z",
     "start_time": "2019-11-24T01:33:47.400930Z"
    }
   },
   "outputs": [],
   "source": [
    "# draw the loss, pred_rate with iteration andd the lr with epoch\n",
    "plt. figure(figsize=(30,10))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.plot(np.arange(1,len(pred_rate_ens)+1),pred_rate_ens)\n",
    "plt.title('prediction rate')\n",
    "plt.xlabel('iteration')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.plot(np.arange(1,len(loss_ens)+1),loss_ens)\n",
    "plt.title('loss')\n",
    "plt.xlabel('iteration')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.plot(np.arange(num_epoches+1),lr_ens)\n",
    "plt.title('learning rate')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T01:26:11.720934Z",
     "start_time": "2019-11-24T01:26:11.692013Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'resnet9_piecewise_linear.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T01:26:13.979896Z",
     "start_time": "2019-11-24T01:26:11.875537Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model = ResNet(ResidualBlock, [2, 2]).to(device)\n",
    "model.load_state_dict(torch.load('resnet9_piecewise_linear.ckpt'))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():        \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
